{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rash-rc/credit-risk-modeling/blob/google-colab-notebook/source.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3djtEfyh0zLZ"
      },
      "source": [
        "### Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VIlYO-KWPSO9"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QocbBovW0zLb"
      },
      "outputs": [],
      "source": [
        "#import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_P2dZGW0zLb"
      },
      "outputs": [],
      "source": [
        "#import data\n",
        "loan_data_backup = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/dataset/loan.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eogd4zVv0zLc"
      },
      "outputs": [],
      "source": [
        "loan_data = loan_data_backup.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNWxBE470zLc"
      },
      "outputs": [],
      "source": [
        "pd.options.display.max_columns = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAq1RMFL0zLc"
      },
      "outputs": [],
      "source": [
        "#loan_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XEN1tQh0zLd"
      },
      "outputs": [],
      "source": [
        "loan_data.head() #First 5 rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eK_W2vHi0zLd"
      },
      "outputs": [],
      "source": [
        "loan_data.tail() #Last 5 rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2D_wDvJz0zLd"
      },
      "outputs": [],
      "source": [
        "loan_data.columns.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vuuS4kKI0zLd"
      },
      "outputs": [],
      "source": [
        "loan_data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ln-31wQ0zLd"
      },
      "outputs": [],
      "source": [
        "loan_data.dtypes.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aduBPH_N0zLd"
      },
      "outputs": [],
      "source": [
        "loan_data.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KaYMKO90zLd"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_rows', None)  # Show all rows (columns in this case)\n",
        "print(loan_data.dtypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ItWedXx0zLe"
      },
      "outputs": [],
      "source": [
        "columns_info = loan_data.dtypes.reset_index()\n",
        "columns_info.columns = ['Column Name', 'Data Type']\n",
        "print(columns_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqRWBV6x0zLe"
      },
      "outputs": [],
      "source": [
        "for col in loan_data.columns:\n",
        "    print(f\"Column: {col}, Data Type: {loan_data[col].dtype}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJQFJvhf0zLe"
      },
      "source": [
        "### General Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaJ4_8KQ0zLe"
      },
      "source": [
        "#### Preprocessing few continuos variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1R0ugeKK0zLe"
      },
      "outputs": [],
      "source": [
        "loan_data['emp_length'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEiJfUcQ0zLe"
      },
      "outputs": [],
      "source": [
        "loan_data['emp_length_int'] = loan_data['emp_length'].str.replace(r'\\+ years', '', regex = True)\n",
        "loan_data['emp_length_int'] = loan_data['emp_length_int'].str.replace('< 1 year', str(0))\n",
        "loan_data['emp_length_int'] = loan_data['emp_length_int'].fillna(str(0))\n",
        "loan_data['emp_length_int'] = loan_data['emp_length_int'].str.replace('years', '')\n",
        "loan_data['emp_length_int'] = loan_data['emp_length_int'].str.replace('year', '')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ssuPZUfa0zLe"
      },
      "outputs": [],
      "source": [
        "loan_data['emp_length_int'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uowI-rD40zLe"
      },
      "outputs": [],
      "source": [
        "type(loan_data['emp_length_int'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gM-ebf160zLe"
      },
      "outputs": [],
      "source": [
        "loan_data['emp_length_int'] = pd.to_numeric(loan_data['emp_length_int'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fk2gR2sr0zLe"
      },
      "outputs": [],
      "source": [
        "type(loan_data['emp_length_int'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MN4hY5CJ0zLf"
      },
      "outputs": [],
      "source": [
        "loan_data['term'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gn1KVaP20zLf"
      },
      "outputs": [],
      "source": [
        "loan_data['term_int'] = loan_data['term'].str.replace('months', '')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYDF28oU0zLf"
      },
      "outputs": [],
      "source": [
        "loan_data['term_int'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rT1-_OLn0zLf"
      },
      "outputs": [],
      "source": [
        "loan_data['term_int'] = pd.to_numeric(loan_data['term_int'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7rlK2yI0zLf"
      },
      "outputs": [],
      "source": [
        "type(loan_data['term_int'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZopybQc0zLf"
      },
      "outputs": [],
      "source": [
        "loan_data['earliest_cr_line'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vr0kRc490zLf"
      },
      "outputs": [],
      "source": [
        "#loan_data['earliest_cr_line']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q80G04Tu0zLf"
      },
      "outputs": [],
      "source": [
        "loan_data['earliest_cr_line_date'] = pd.to_datetime(loan_data['earliest_cr_line'], format = '%b-%y')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8yO-sR_0zLf"
      },
      "outputs": [],
      "source": [
        "#loan_data['earliest_cr_line_date']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6WatDuB50zLg"
      },
      "outputs": [],
      "source": [
        "type(loan_data['earliest_cr_line_date'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MxmN5-Jy0zLg"
      },
      "outputs": [],
      "source": [
        "maxdate = loan_data['earliest_cr_line_date'].max()\n",
        "maxdate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UrGyxLCf0zLg"
      },
      "outputs": [],
      "source": [
        "mindate = loan_data['earliest_cr_line_date'].min()\n",
        "mindate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6hsR1kv0zLg"
      },
      "outputs": [],
      "source": [
        "#In order to use the credit line data in regression we need the time since the earliest credit line was issued. We need a reference date in order to do that\n",
        "\n",
        "#Ususally we would take the current date but since our data is older we are assuming it as December 2015\n",
        "\n",
        "pd.to_datetime('2017-12-01') - loan_data['earliest_cr_line_date']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVYtxRoV0zLg"
      },
      "outputs": [],
      "source": [
        "#Conventinally months are used ( 'M' is deprecated thus, we will first convert into days and then month)\n",
        "#delta = np.timedelta64(100, 'D')\n",
        "reference_date = pd.to_datetime('2017-12-01')\n",
        "loan_data['months_since_earliest_cr_line'] = round(pd.to_numeric((reference_date - pd.to_datetime(loan_data['earliest_cr_line_date']))/ np.timedelta64(1, 'D'))/30.417)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVN4ZLxM0zLh"
      },
      "outputs": [],
      "source": [
        "#loan_data['months_since_earliest_cr_line']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_rsHXUh0zLh"
      },
      "outputs": [],
      "source": [
        "loan_data['months_since_earliest_cr_line'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NvBM3dyv0zLh"
      },
      "outputs": [],
      "source": [
        "# In the above given stats we see min as negative ( negative time difference), it needs to be addressed as it;s not possible to have negative days\n",
        "\n",
        "#Start by displaying data points where the negative time differences were calculated, we can select specific rows & columns of a pandas data frame by their labels using the loc method.\n",
        "\n",
        "loan_data.loc[:, ['earliest_cr_line', 'earliest_cr_line_date', 'months_since_earliest_cr_line']][loan_data['months_since_earliest_cr_line']<0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0CdslB9x0zLh"
      },
      "outputs": [],
      "source": [
        "#In the above output we see that the earliest credit line date is after the reference date (December 2017), which is not possible.\n",
        "#This could have been a miss in conversion where 1967 was interpreted as 2067 - However this conversion coukd be time consuming.\n",
        "#Additionally, it is most likely the issue arose in the first place because the origin of the built-in time scale starts after 1970.\n",
        "#One solution can be to remove data, but that would lead to loss of data, since data is important and we wouldn't want to remove it so easily.\n",
        "#Solution we are going with is to impute the negative values, but with what ?\n",
        "#We know that we get the negative values for the credit line issues at a very distant pointy in the past, in the 60s, that is a longer period than all other credit lines that we normally get values for\n",
        "# - So, we could substitute the negative values with the maximum observed, normal or positive difference. That way, even if we don't claculate the exact number of months\n",
        "# - that have passed since the earliest credit line was issued for those issued in the 60s, we put a very large value and we still get pretty close to the real picture.\n",
        "\n",
        "loan_data['months_since_earliest_cr_line'][loan_data['months_since_earliest_cr_line'] < 0] = loan_data['months_since_earliest_cr_line'].max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J8H8Jujl0zLh"
      },
      "outputs": [],
      "source": [
        "min(loan_data['months_since_earliest_cr_line'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MPNcIsv30zLh"
      },
      "outputs": [],
      "source": [
        "loan_data['issue_d'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vG95_NR0zLh"
      },
      "outputs": [],
      "source": [
        "loan_data['issue_d_date'] = pd.to_datetime(loan_data['issue_d'], format = '%b-%y')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9-umruL0zLi"
      },
      "outputs": [],
      "source": [
        "#loan_data['issue_d_date']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5h_HAku0zLi"
      },
      "outputs": [],
      "source": [
        "loan_data['issue_d_date'].min()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MXngZ6HE0zLi"
      },
      "outputs": [],
      "source": [
        "loan_data['issue_d_date'].max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pb3vCs1i0zLi"
      },
      "outputs": [],
      "source": [
        "loan_data['months_since_issue_date'] = round(pd.to_numeric(pd.to_datetime('2017-12-01') - loan_data['issue_d_date']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZdXyCsS0zLi"
      },
      "outputs": [],
      "source": [
        "#loan_data['months_since_issue_date']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9tCSeyay0zLi"
      },
      "outputs": [],
      "source": [
        "reference_date = pd.to_datetime('2017-12-01')\n",
        "# loan_data['months_since_issue_date'] = round(pd.to_numeric(pd.to_datetime('2017-12-01') - loan_data['issue_d_date']))\n",
        "loan_data['months_since_issue_date'] = round(pd.to_numeric((reference_date - pd.to_datetime(loan_data['issue_d_date']))/ np.timedelta64(1, 'D'))/30.417)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oESUi-Vh0zLi"
      },
      "outputs": [],
      "source": [
        "#loan_data['months_since_issue_date']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urTecC9u0zLi"
      },
      "outputs": [],
      "source": [
        "loan_data['months_since_issue_date'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44Xl2cV20zLi"
      },
      "outputs": [],
      "source": [
        "#Preprocessing discrete variables\n",
        "#grade, sub_grade, home_ownership, verification_status, loan_status, purpose, addr_state, initial_list_status\n",
        "\n",
        "loan_data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQLRS4j50zLi"
      },
      "source": [
        "With discrete features we would want to create  dummy variables for all of their categories. Dummy variables are binary indicators: 1, if an observation belongs to a category; 0, if it does not ( Eg., gender - F/M, for this information to be useful for a statistical model, it has to be numerically represented by dummy variables)\n",
        "\n",
        "We need only k-1 dummy variables to represent the information about k categories.\n",
        "\n",
        "Note: It will be best to create a new dataframe where we will store all the new dummy variables and then concatenate it to the loan_data dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HqHl_cwX0zLi"
      },
      "outputs": [],
      "source": [
        "#pandas has a built-in function to create dummy variables for a given categorical variable - pd.get_dummies()\n",
        "\n",
        "# pd.get_dummies(loan_data['grade'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IGaSHzkc0zLi"
      },
      "outputs": [],
      "source": [
        "loan_data['grade'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxigVR_W0zLi"
      },
      "outputs": [],
      "source": [
        "dummies = pd.get_dummies(loan_data['grade'], prefix = 'grade', prefix_sep = ':')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2uBSufBf0zLi"
      },
      "outputs": [],
      "source": [
        "dummies.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0JbaKJ90zLi"
      },
      "outputs": [],
      "source": [
        "sample = loan_data['grade'].sample(1000, random_state=42)\n",
        "pd.get_dummies(sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xnY2OGrH0zLi"
      },
      "outputs": [],
      "source": [
        "loan_data_dummies = [pd.get_dummies(loan_data['grade'], prefix = 'grade', prefix_sep = ':'),\n",
        "                     pd.get_dummies(loan_data['sub_grade'], prefix = 'sub_grade', prefix_sep = ':'),\n",
        "                     pd.get_dummies(loan_data['home_ownership'], prefix = 'home_ownership', prefix_sep = ':'),\n",
        "                     pd.get_dummies(loan_data['verification_status'], prefix = 'verification_staus', prefix_sep = ':'),\n",
        "                     pd.get_dummies(loan_data['loan_status'], prefix ='loan_status', prefix_sep = ':'),\n",
        "                     pd.get_dummies(loan_data['purpose'], prefix = 'purpose', prefix_sep = ':'),\n",
        "                     pd.get_dummies(loan_data['addr_state'], prefix = 'addr_state', prefix_sep = ':'),\n",
        "                     pd.get_dummies(loan_data['initial_list_status'], prefix = 'initial_list_status', prefix_sep = ':')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2KSWU9J0zLj"
      },
      "outputs": [],
      "source": [
        "loan_data_dummies = pd.concat(loan_data_dummies, axis =1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROt8R_xR0zLj"
      },
      "outputs": [],
      "source": [
        "type(loan_data_dummies)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5ZE9Yvd0zLj"
      },
      "source": [
        "We need to specify whether we want to concatenate the inputs by rows or columns. We do that with the axis parameter.\n",
        "By default, axis=0, which means that the inputs are concatenated by rows. If we want to concatenate by columns, we need to set axis=1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KRVCSRSL0zLj"
      },
      "outputs": [],
      "source": [
        "loan_data = pd.concat([loan_data, loan_data_dummies], axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZK0GjY40zLj"
      },
      "outputs": [],
      "source": [
        "loan_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PE4AA9zI0zLj"
      },
      "outputs": [],
      "source": [
        "loan_data.columns.values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tF75gvQe0zLj"
      },
      "source": [
        "Check for missing values and clean"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOq2O3th0zLj"
      },
      "source": [
        "A dedicated pandas method df.isnull, is used to check if each data point is missing (True) or not(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4AD5d-d0zLj"
      },
      "outputs": [],
      "source": [
        "#loan_data.isnull()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-oTHMFi0zLj"
      },
      "outputs": [],
      "source": [
        "loan_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xesiEWsF4zkG"
      },
      "outputs": [],
      "source": [
        "# Show all rows that have at least one null value\n",
        "null_rows = loan_data[loan_data.isnull().any(axis=1)]\n",
        "display(null_rows.head(20))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2xdivnt8L7S"
      },
      "outputs": [],
      "source": [
        "pd.options.display.max_rows = None\n",
        "loan_data.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elXvNYeQ6u-V"
      },
      "source": [
        "One way to deal with missing values is to remove all observations(rows) where we have missing value, another way is to impute them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-uHOm8iAbvQ7"
      },
      "outputs": [],
      "source": [
        "pd.options.display.max_rows = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZ9MGMM-7GrC"
      },
      "outputs": [],
      "source": [
        "#Total revolving limit, we use fillna, there is a need to specify two arguments. One of the missing values can be the value we want to replace missing values with, we take the funded amount. If missing values needs to be replaced in the same variable we set inplace = True\n",
        "loan_data['total_rev_hi_lim'].fillna(loan_data['funded_amnt'], inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HUtHKcZs9sg0"
      },
      "outputs": [],
      "source": [
        "pd.options.display.max_rows = None\n",
        "loan_data['total_rev_hi_lim'].isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQQT8pU4-nqY"
      },
      "outputs": [],
      "source": [
        "loan_data['annual_inc'].fillna(loan_data['annual_inc'].mean(), inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dw8oFdQC-7KW"
      },
      "outputs": [],
      "source": [
        "loan_data['annual_inc'].isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_qDGy-e-_zd"
      },
      "outputs": [],
      "source": [
        "#Replacing the missing values with zeroes\n",
        "loan_data['months_since_earliest_cr_line'].fillna(0, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y9VvaUv8JhgL"
      },
      "outputs": [],
      "source": [
        "loan_data['months_since_earliest_cr_line'].isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DA__q-68Ju8n"
      },
      "outputs": [],
      "source": [
        "loan_data['acc_now_delinq'].fillna(0, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47SNF-00J4MQ"
      },
      "outputs": [],
      "source": [
        "loan_data['acc_now_delinq'].isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLQJB4NxJ8nC"
      },
      "outputs": [],
      "source": [
        "loan_data['total_acc'].fillna(0, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mBbnSuGwKEig"
      },
      "outputs": [],
      "source": [
        "loan_data['total_acc'].isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k2iugfglKHAe"
      },
      "outputs": [],
      "source": [
        "loan_data['pub_rec'].fillna(0, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGEBKz_DKS_L"
      },
      "outputs": [],
      "source": [
        "loan_data['pub_rec'].isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LRdTbCGjKVZU"
      },
      "outputs": [],
      "source": [
        "loan_data['open_acc'].fillna(0, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AS-cY3OlKoyh"
      },
      "outputs": [],
      "source": [
        "loan_data['open_acc'].isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRfAFXxsKr7M"
      },
      "outputs": [],
      "source": [
        "loan_data['inq_last_6mths'].fillna(0, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9l5qZmqnK3Rr"
      },
      "outputs": [],
      "source": [
        "loan_data['inq_last_6mths'].isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PcF25AifK827"
      },
      "outputs": [],
      "source": [
        "loan_data['delinq_2yrs'].fillna(0, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LG30SJ20LHBl"
      },
      "outputs": [],
      "source": [
        "loan_data['delinq_2yrs'].isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_ipLYCuLLi_"
      },
      "outputs": [],
      "source": [
        "loan_data['emp_length_int'].fillna(0, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OH8osR1ZLUEo"
      },
      "outputs": [],
      "source": [
        "loan_data['emp_length_int'].isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGbfFywhIBXT"
      },
      "source": [
        "# Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEoaLUUYLbPy"
      },
      "outputs": [],
      "source": [
        "# Expected Loss (EL) = Probability of default (PD) * Loss given default (LGD) * Exposure at default (EAD)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeVhPHKUXGE_"
      },
      "source": [
        "We will need to establish our dependent variable i.e., what we are going to predict. Our interest lies in the borrowers and their respective loans on which they have defaulted or not.\n",
        "- Loans defaulted - Bad\n",
        "- Performed well - Good\n",
        "(Default definition aka good or bad definition -- Rules set to categorize a client as defaulter -- They are typically based on the delinquency of the borrower, measured in days past the payment due date eg., 90 days / if comitted fraud)\n",
        "\n",
        "The established statistical methodology to model probability of default is a logistic regression where the dependent variable is precisely whether a customer defaulted or not.\n",
        "\n",
        "Logistic regression estimates the relationship between two things.\n",
        "The loagarithm of odds of an outcome of interest (non default or default) or dependent variable and a linear combination of predictors or independent variables.\n",
        "\n",
        "In a PD model, interpretability is extremely important as it is required by the regulators i.e., the model must be very easy to understand and apply. Which is why the established practice is for all independent variables in the probability of default models to be dummy variables (binary categorical variables or indicator variables).\n",
        "Here, both discrete and continuous variables need to be dummy variables.\n",
        "After we do that, the PD model will be logistic regression model with a binary indicator for good or bad or non-default or default as dependent variables and only dummy variables as independent variables.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKwGeQxVcE4o"
      },
      "source": [
        "## PD Model\n",
        "## Data Preparation\n",
        "### Dependent variable. Good/Bad (Default) /definition. Default and Non-default Accounts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPO4lkF_LhQb"
      },
      "outputs": [],
      "source": [
        "loan_data['loan_status'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XwRSl5gqcr8T"
      },
      "outputs": [],
      "source": [
        "#To check how many accounts exist for each status\n",
        "loan_data['loan_status'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AeNddZdgdPe8"
      },
      "outputs": [],
      "source": [
        "#It is useful to see the proportion of accounts by status\n",
        "loan_data['loan_status'].value_counts()/ loan_data['loan_status'].count()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05_cCWK0d5um"
      },
      "outputs": [],
      "source": [
        "# np.where(condition, value if true, value if false)\n",
        "\n",
        "loan_data['good_bad'] = np.where(loan_data['loan_status'].isin(['Charged off', 'Default', 'Does not meet the credit policy. Status:Charged Off', 'Late (31-120 days)' ]), 0, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sd_BsZeyfVDv"
      },
      "outputs": [],
      "source": [
        "#loan_data['good_bad']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcBnTI9-gwrW"
      },
      "source": [
        "For discrete variables like external ratings, purpose of the loan, home ownership and so on, we can use their categories straight away. If we have too many categories or too similar categories, we can bundle up several dummies into one.\n",
        "\n",
        "Continuous variables also need to be changed into dummy variables - How do we know where one category ends and another starts?\n",
        "Eg., annual income 0 - 200k. One way is to bundle them 0-75K - category 1, 75k - 95k another and so on .\n",
        "\n",
        "There is a well-established methodology to turn continuous variables into categories, once they are categorical we proceed by bundling them up depending on their properties. This process is called fine-classing.\n",
        "\n",
        "So conceptually, through fine-classing, both discrete & continuous values can be represented with categories. How do we run these arbitrary categories into good, usable dummies?\n",
        "Since we will be dealing with categories, in both cases, the approach will be the same.\n",
        "We start by getting some rough initial assessments of the ability of each category to predict the dependent variable. There are many ways however, the established metric that is conventionally used is weight of evidence.\n",
        "Weight of evidence shows to what extent an independent variable would predict a dependent variable. in other words, how much evidence does the independent variable have with respect to differences in the dependent variable?\n",
        "More specifically, weight of evidence shows the extent to which each of the different categories of an independent variable explains the dependent one.\n",
        "\n",
        "Weight of evidence is the natural logarithm of the ratio of the proportion of observations of the first type of outcome of the dependent variable that fall into the respective category of the independent variable and the proportion of observations of the second type of outcome of the dependent variable that fall into the respective category of the independent variable.\n",
        "\n",
        "Coarse classing is the process of constructing new categories based on the initial ones.\n",
        "\n",
        "Additional Note : Informational Value (IV)\n",
        "\n",
        "Range 0-1 - Predictive Powers\n",
        "IV < 0.02 - No predictive power\n",
        "0.02 < IV < 0.1 - Weak predictive power\n",
        "0.1 < IV < 0.3 - Medium predictive power\n",
        "0.3 < IV < 0.5 - strong predictive power\n",
        "0.5 < IV - Suspisciously high, too good to be true\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRE7bQyKGGoT"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaF6dQNaGLEy"
      },
      "source": [
        "Before we start pre-processing our variables, we need to make sure we have the proper setup to estimate our probability of default modle in the best way possible. a substantial issue we might face while estimating any statistical model is overfitting - Our statistical model has focused on a particular dataset so much that it has missed the point. Opposite of which is underfitting, where the modle fails to capture the underlying logic of the data ie., it didn't learn well, so it does not know what to do and therefore it provides inaccurate answers.\n",
        "Underfitting (Doesn't capture logic) is easier to spot, as you've no accuracy whatsoever. Overfitting (captures all the noise) is much harder though, as the accuracy of the model seems outstanding.\n",
        "Basic solution for overfitting is to split our initial dataset into two training and test splits. (10:90 - test:train or 20:80 - are common)\n",
        "\n",
        "- Basically, we hide a small part of the dataset from the algorithm, so we train the model based on most of the data not all of it. After we have the model, we test it on the test data by creating a confusion matrix and assessing the accuracy. The whole point is that the modle has never seen the test dataset. Therefore it cannot overfit on it. Sklearn offers a pretty neat method of splitting the data into train and test.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUVLEm1ufZ0g"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qbz61uWtJ8WY"
      },
      "outputs": [],
      "source": [
        "#train_test_split is very powerful and has many arguments we can take advantage of. Most important are input and target dataframes.\n",
        "#train_test_split(loan_data.drop('good_bad', axis = 1), loan_data['good_bad])']\n",
        "\n",
        "#The output is 4 arrays: 1. train dataset with inputs, 2. test dataset with inputs, 3. train dataset with targets, 4. test dataset with targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3Q07AnNK306"
      },
      "outputs": [],
      "source": [
        "loan_data_inputs_train, loan_data_inputs_test, loan_data_targets_train, loan_data_targets_test = train_test_split(loan_data.drop('good_bad', axis = 1), loan_data['good_bad'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-QxPMR5Ha_K"
      },
      "outputs": [],
      "source": [
        "loan_data_inputs_train.shape # training inputs contain 349,713 observations along 207 variables"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loan_data_targets_train.shape # targets are a vector of length containing 349,713 observations"
      ],
      "metadata": {
        "id": "e9nQ5rDSZvGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loan_data_inputs_test.shape # training inputs contain 116,572 observations along 207 variables"
      ],
      "metadata": {
        "id": "_kQVtPRLZ5BN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loan_data_targets_test.shape # targets are a vector of length containing 116,572 observations"
      ],
      "metadata": {
        "id": "ZcaTmXKgaBn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split we have Train : Test = 349713:116572 = 3:1 = 75% : 25% - Default Split. Usually though we opt for splits like 90%:10% (0.1)or 80%:20% (0.2). Setting aside too much data for testing means training the model on less data. For that we can specify the test size.\n",
        "train_test method has a shuffle parameter, shuffle is a boolean, so it can be either true or false ( by default it is set to true).\n",
        "Sklearn is smart and it handles shuffling too, which unfortunately is a small issue for us. When we rerun the code, we get a different shuffle and this means a different split. This causes the final model to differ every time due to the shuffling. So, we get slightly different accuracy every time and slightly different model coefficients too. With that said, this automatic shuffling can make things difficult for us. There is a solution for this though, all sklearn functions that include some randomness contain a random_state parameter and it takes integer values. What we can do is set the random stste to a number, say 42. This will make the shuffle pseudo random i.e. it will always shuffle the model in the same random way."
      ],
      "metadata": {
        "id": "hCf_lG_YcgoC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loan_data_inputs_train, loan_data_inputs_test, loan_data_targets_train, loan_data_targets_test = train_test_split(loan_data.drop('good_bad', axis = 1), loan_data['good_bad'], test_size = 0.2, random_state = 42)"
      ],
      "metadata": {
        "id": "kF0LSL3uaFfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loan_data_inputs_train.shape"
      ],
      "metadata": {
        "id": "OBy6UIWvcF4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loan_data_targets_train.shape"
      ],
      "metadata": {
        "id": "nCaOwPndggu-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loan_data_inputs_test.shape"
      ],
      "metadata": {
        "id": "sN1geDngglFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loan_data_targets_test.shape"
      ],
      "metadata": {
        "id": "gFfCuxlagqNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split we have here is Train:Test = 373,028:93,257 = 4:1 = 80%:20%"
      ],
      "metadata": {
        "id": "VLronL79g6t-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preparation: An Example"
      ],
      "metadata": {
        "id": "nbdRNilLpsMr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Discrete variables do not need fine classing(splitting into categories) because they are already categorical by definition."
      ],
      "metadata": {
        "id": "mYy52WlitOvr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_inputs_prepr = loan_data_inputs_train\n",
        "df_targets_prepr = loan_data_targets_train"
      ],
      "metadata": {
        "id": "mcXXkh-hguRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We are assessing its explanatory power with respect to the outcome of interest that is being a good or a bad borrower. We also need the corresponding outcomes contained in the good bad column."
      ],
      "metadata": {
        "id": "CPw8VfXBprQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_inputs_prepr['grade'].unique()"
      ],
      "metadata": {
        "id": "fSPoTr2Ig6E-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We will create another dataframe, where we will store only the independent variable grade from the df_inputs_prep abd the dependent variable good, bad from the df_targets_prepr"
      ],
      "metadata": {
        "id": "mZ9vZt3oup2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = pd.concat([df_inputs_prepr['grade'], df_targets_prepr], axis = 1)\n",
        "df1.head()"
      ],
      "metadata": {
        "id": "CcFb7ycQvaPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to calculate weight of evidence of the discrete variable here, WoE = ln(%Good/%Bad). We first find the proportion of good and bad borrowers individually(by grade).\n",
        "\n",
        "- Let's start with number of borrowers in each grade.\n",
        "- To do that, we can count the rows that contain each of the grades.\n",
        "- We do this with the group by method.\n",
        "- Group by splits the data according to certain criteria.\n",
        "(In our case, we want to split by grade)"
      ],
      "metadata": {
        "id": "WmBzmxrZwRcN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1.groupby('grade').count()\n",
        "# but since we want this code to be reusable, let's parameterize it. It is the first column in our df, the counting in python starts from 0, thus index of first column is 0\n",
        "# df1.groupby(df1.columns.values[0]) - If we group like this, the grouped values become indexes in the result we get. Having them as indicies is great but rather limiting,\n",
        "# so it's better to have them as ordinary values. This is something we must specify explicitly in the group by method, by setting the as_index parameter to false"
      ],
      "metadata": {
        "id": "7exIaEn4vu_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.groupby(df1.columns.values[0], as_index = False)[df1.columns.values[1]].count()"
      ],
      "metadata": {
        "id": "rEM43VhXx2Kc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Note good borrower has a value 1 and bad has 0, average suffices for good"
      ],
      "metadata": {
        "id": "bJ7ubREQ0B6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.groupby(df1.columns.values[0], as_index = False)[df1.columns.values[1]].mean()\n",
        "df1"
      ],
      "metadata": {
        "id": "VYgoTIa30afm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = pd.concat([df1.groupby(df1.columns.values[0], as_index = False)[df1.columns.values[1]].count(),\n",
        "                df1.groupby(df1.columns.values[0], as_index = False)[df1.columns.values[1]].mean()], axis = 1)"
      ],
      "metadata": {
        "id": "ea52-TeG0yB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1"
      ],
      "metadata": {
        "id": "SVfKoYUK18gP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = df1.iloc[:, [0,1,3]]\n",
        "df1"
      ],
      "metadata": {
        "id": "SMDfWCIi197K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#names are suboptimal , so let us change them to make it more instructive."
      ],
      "metadata": {
        "id": "0-vzTVxQ2KXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.columns = [df1.columns.values[0], 'n_obs', 'prop_good']\n",
        "df1"
      ],
      "metadata": {
        "id": "cVRiP7U52eJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1['prop_n_obs'] = df1['n_obs']/ df1['n_obs'].sum()"
      ],
      "metadata": {
        "id": "9vNmyvvO2tRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1"
      ],
      "metadata": {
        "id": "caR-SeRm3Dna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1['n_good'] = df1['prop_good'] * df1['n_obs']\n",
        "df1['n_bad'] = (1 - df1['prop_good']) * df1['n_obs']\n",
        "df1"
      ],
      "metadata": {
        "id": "aAjU74PD3EM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1['prop_n_good'] = df1['n_good']/ df1['n_good'].sum()\n",
        "df1['prop_n_bad'] = df1['n_bad']/ df1['n_bad'].sum()\n",
        "df1"
      ],
      "metadata": {
        "id": "MwwJ175F3rvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1['WoE'] = np.log(df1['prop_n_good']/df1['prop_n_bad'])"
      ],
      "metadata": {
        "id": "YgE3TtL04FR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1"
      ],
      "metadata": {
        "id": "tcH9y1vY4loq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = df1.sort_values(['WoE'])\n",
        "df1 = df1.reset_index(drop = True)\n",
        "df1\n",
        "#This way we can see the categories where borrowers have the higest default rate first"
      ],
      "metadata": {
        "id": "gpCWIMjV4wUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For PD models, I would also like to calculate the differences in the proportion of good loans between two subsequent categories and the difference of weight of evidence between two subsequent categories.\n",
        "# We shall use diff to calculate them, it is used to calculate the difference of each two subsequent rows\n",
        "# Here we'll get negative values because the function substracts the value of one row from the value of the row above. It is more intuitive to see positive values as they show how much woe would increase from one class to the next.\n",
        "# Thus we'll take the absolute value of that difference."
      ],
      "metadata": {
        "id": "dK4UdvnN47gy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1['diff_prop_good'] = df1['prop_good'].diff().abs()\n",
        "df1['diff_WoE'] = df1['WoE'].diff().abs()"
      ],
      "metadata": {
        "id": "9dSQKbw257B-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1"
      ],
      "metadata": {
        "id": "dsX7Hofh5-dR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Information Value\n",
        "df1['IV'] = (df1['prop_n_good'] - df1['prop_n_bad']) * df1['WoE']\n",
        "df1['IV'] = df1['IV'].sum()\n",
        "df1"
      ],
      "metadata": {
        "id": "FgHCL0JN6AAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "45jAv_y965o8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}